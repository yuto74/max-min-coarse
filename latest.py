#!/usr/bin/env python3
#
# Copyright (c) 2024, BW-COARSE implementation by Gemini
# Based on run_kmeans_cluster_coarse.py
#
# BW-COARSE: A graph coarsening program that preserves max-min fair bandwidth allocation.
# This program implements the concepts described in the provided research paper.
#

import argparse
import math
from collections import defaultdict, deque
import heapq # heapqã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ 

import dgl
import graph_tools
import networkx as nx
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.cluster import KMeans

# s_t_flow.pyã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒªãƒ³ã‚¯å¸¯åŸŸã‚’å®šæ•°ã¨ã—ã¦å®šç¾©
DEFAULT_LINK_BANDWIDTH = 1000

def import_graph(file_path: str) -> tuple[graph_tools.Graph, dict]:
    """
    Load a graph in DOT format from a file using graph_tools.
    It also normalizes node IDs to be consecutive integers starting from 0,
    while preserving original names in a vertex attribute.
    Returns the graph and a map from original names to new integer IDs.
    """
    print(f"Step 0: å…¥åŠ›ã‚°ãƒ©ãƒ• {file_path} ã‚’ graph_tools ã§èª­ã¿è¾¼ã¿ä¸­...")
    g = graph_tools.Graph(directed=False)
    with open(file_path) as f:
        lines = f.readlines()
        g.import_dot(lines)
    
    # Map original node names to consecutive integer IDs for stable processing
    try:
        # æ•°å€¤ã®ãƒãƒ¼ãƒ‰åã‚’æ•´æ•°ã¨ã—ã¦ã‚½ãƒ¼ãƒˆ
        sorted_vertices = sorted(g.vertices(), key=int)
    except ValueError:
        # æ–‡å­—åˆ—ã®ãƒãƒ¼ãƒ‰åã‚’æ–‡å­—åˆ—ã¨ã—ã¦ã‚½ãƒ¼ãƒˆ
        sorted_vertices = sorted(g.vertices(), key=str)
        
    node_map = {str(name): i for i, name in enumerate(sorted_vertices)}
    
    # Create a new graph with integer IDs
    h = graph_tools.Graph(directed=False)
    for orig_name, new_name in node_map.items():
        h.add_vertex(new_name)
        h.set_vertex_attribute(new_name, 'original_name', orig_name)

    for u_orig, v_orig in g.edges():
        h.add_edge(node_map[str(u_orig)], node_map[str(v_orig)])
        
    print(f"  ã‚°ãƒ©ãƒ•ã®èª­ã¿è¾¼ã¿ã¨ãƒãƒ¼ãƒ‰IDã®æ­£è¦åŒ–ãŒå®Œäº†ã€‚ãƒãƒ¼ãƒ‰æ•°: {h.nvertices()}")
    return h, node_map

def graph_tools_to_networkx(g_tools: graph_tools.Graph) -> nx.Graph:
    """
    Convert a graph_tools.Graph object to a networkx.Graph object.
    """
    g_nx = nx.Graph()
    g_nx.add_nodes_from(g_tools.vertices())
    g_nx.add_edges_from(g_tools.edges())
    return g_nx

def create_node_features(g: graph_tools.Graph, node_map: dict, flow_file: str) -> torch.Tensor:
    """
    Create node features using graph metrics and flow data.
    These features are designed to capture the bandwidth structure as described in the paper.
    This version includes additional graph-theoretic features.
    """
    print("Step 1: ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ã‚’ç”Ÿæˆä¸­...")

    # === 1. Calculate structural and graph-theoretic features ===
    print("    æ§‹é€ çš„ç‰¹å¾´é‡ãŠã‚ˆã³ã‚°ãƒ©ãƒ•ç†è«–çš„ç‰¹å¾´é‡ã‚’è¨ˆç®—ä¸­...")
    for v in g.vertices():
        # æ¬¡æ•° (æ—¢å­˜)
        degree = g.degree(v)
        g.set_vertex_attribute(v, 'degree', degree)
        
        # â˜…æ–°è¦è¿½åŠ : ä»‹åœ¨ä¸­å¿ƒæ€§ (Betweenness Centrality)
        #    æ­£è¦åŒ–ã•ã‚ŒãŸå€¤ã‚’è¨ˆç®—
        betweenness = g.betweenness_centrality(v, normalize=True)
        g.set_vertex_attribute(v, 'betweenness', betweenness)

        # â˜…æ–°è¦è¿½åŠ : å±€æ‰€ã‚¯ãƒ©ã‚¹ã‚¿ä¿‚æ•° (Local Clustering Coefficient)
        #    æ¬¡æ•°ãŒ2æœªæº€ã®ãƒãƒ¼ãƒ‰ã¯è¨ˆç®—ã§ããªã„ãŸã‚0ã¨ã™ã‚‹
        if degree > 1:
            # ntriadsã¯ãƒãƒ¼ãƒ‰vã®è¿‘å‚é–“ã«å­˜åœ¨ã™ã‚‹ã‚¨ãƒƒã‚¸æ•°ã‚’æ•°ãˆã‚‹
            local_clustering = g.ntriads(v) / ((degree * (degree - 1)) / 2)
        else:
            local_clustering = 0.0
        g.set_vertex_attribute(v, 'local_clustering', local_clustering)


    # === 2. Calculate flow-based features (æ—¢å­˜ã®å‡¦ç†) ===
    print(f"    ãƒ•ãƒ­ãƒ¼ãƒ‡ãƒ¼ã‚¿ {flow_file} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    flow_out_bw = defaultdict(float)
    flow_in_bw = defaultdict(float)
    flow_out_count = defaultdict(int)
    flow_in_count = defaultdict(int)
    passing_through_bw = defaultdict(float)
    passing_through_count = defaultdict(int)

    flows = []
    with open(flow_file, 'r') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) == 3:
                src, dst, bw = parts
                bw = float(bw)
                flows.append((src, dst, bw))
                
                # Aggregate endpoint features
                flow_out_bw[src] += bw
                flow_in_bw[dst] += bw
                flow_out_count[src] += 1
                flow_in_count[dst] += 1
    
    print("    é€šéãƒ•ãƒ­ãƒ¼ã®ç‰¹å¾´é‡ã‚’è¨ˆç®—ä¸­...")
    for src, dst, bw in flows:
        try:
            src_id = node_map[src]
            dst_id = node_map[dst]
        except KeyError:
            print(f"      è­¦å‘Š: ãƒ•ãƒ­ãƒ¼ ({src}, {dst}) ã®ãƒãƒ¼ãƒ‰ãŒã‚°ãƒ©ãƒ•ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue

        paths = list(g.shortest_paths(src_id, dst_id))
        if not paths:
            continue
        path = paths[0]
        
        intermediate_nodes = path[1:-1]
        for node_id in intermediate_nodes:
            passing_through_bw[node_id] += bw
            passing_through_count[node_id] += 1

    # Assign all flow data and derived features to each node attribute
    for v in g.vertices():
        orig_name = str(g.get_vertex_attribute(v, 'original_name'))
        f_out_bw = flow_out_bw.get(orig_name, 0)
        f_in_bw = flow_in_bw.get(orig_name, 0)
        p_thr_bw = passing_through_bw.get(v, 0)

        g.set_vertex_attribute(v, 'flow_out_bw', f_out_bw)
        g.set_vertex_attribute(v, 'flow_in_bw', f_in_bw)
        g.set_vertex_attribute(v, 'flow_out_count', flow_out_count.get(orig_name, 0))
        g.set_vertex_attribute(v, 'flow_in_count', flow_in_count.get(orig_name, 0))
        g.set_vertex_attribute(v, 'passing_through_bw', p_thr_bw)
        g.set_vertex_attribute(v, 'passing_through_count', passing_through_count.get(v, 0))

        # â˜…æ–°è¦è¿½åŠ : ä¸­ç¶™ãƒ•ãƒ­ãƒ¼ç‡ (Transit Flow Ratio)
        total_bw = f_in_bw + f_out_bw + p_thr_bw
        if total_bw > 0:
            transit_ratio = p_thr_bw / total_bw
        else:
            transit_ratio = 0.0
        g.set_vertex_attribute(v, 'transit_flow_ratio', transit_ratio)

    # === 3. Compose the final feature vector ===
    features = []
    # â˜…ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã‚’æ›´æ–°
    feature_names = [
        # æ§‹é€ çš„ãƒ»ã‚°ãƒ©ãƒ•ç†è«–çš„ç‰¹å¾´é‡
        'degree', 
        'betweenness',
        'local_clustering',
        # ãƒ•ãƒ­ãƒ¼ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡
        'flow_out_bw', 'flow_in_bw', 
        'flow_out_count', 'flow_in_count',
        'passing_through_bw', 'passing_through_count',
        #'transit_flow_ratio',
    ]
    
    print(f"    ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡: {feature_names}")

    for v in sorted(g.vertices()):
        attrs = g.get_vertex_attributes(v)
        vec = [attrs.get(name, 0) for name in feature_names]
        features.append(vec)
        
    features_tensor = torch.FloatTensor(features)
    
    # Normalize each feature column for stable GCN training
    max_vals = features_tensor.max(axis=0).values
    max_vals[max_vals == 0] = 1.0
    features_tensor = features_tensor / max_vals
    
    return features_tensor

class GCNEmbedder(nn.Module):
    """A simple 2-layer GCN model for generating node embeddings."""
    def __init__(self, in_feats: int, h_feats: int, out_feats: int):
        super(GCNEmbedder, self).__init__()
        self.conv1 = dgl.nn.GraphConv(in_feats, h_feats)
        self.conv2 = dgl.nn.GraphConv(h_feats, out_feats)

    def forward(self, g: dgl.DGLGraph, in_feat: torch.Tensor) -> torch.Tensor:
        h = self.conv1(g, in_feat)
        h = F.relu(h)
        h = self.conv2(g, h)
        return h

def generate_node_embeddings(g_nx: nx.Graph, features: torch.Tensor, embedding_dim: int, hidden_dim: int) -> np.ndarray:
    """Generate node embeddings using the GCN model."""
    print("Step 2: GCNã«ã‚ˆã‚‹ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆä¸­...")
    g_dgl = dgl.from_networkx(g_nx)
    g_dgl = dgl.add_self_loop(g_dgl)
    in_dim = features.shape[1]
    
    if in_dim == 0:
        print("    ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ãŒç©ºã§ã™ã€‚åŸ‹ã‚è¾¼ã¿ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return np.array([])

    model = GCNEmbedder(in_dim, hidden_dim, embedding_dim)
    model.eval()
    with torch.no_grad():
        embeddings = model(g_dgl, features)
    print(f"    {embeddings.shape[0]}å€‹ã®ãƒãƒ¼ãƒ‰ã«å¯¾ã—ã€{embeddings.shape[1]}æ¬¡å…ƒã®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚")
    return embeddings.numpy()

def _group_nodes_by_connectivity_and_cluster(g: graph_tools.Graph, node_to_cluster_map: dict) -> tuple[dict, dict]:
    """
    åŒã˜ã‚¯ãƒ©ã‚¹ã‚¿ã«å±ã—ã€ã‹ã¤æ¥ç¶šã•ã‚Œã¦ã„ã‚‹ãƒãƒ¼ãƒ‰ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã™ã‚‹ã€‚
    """
    print("    Step 3a: ãƒªãƒ³ã‚¯æ¥ç¶šã¨ã‚¯ãƒ©ã‚¹ã‚¿ã«åŸºã¥ãã€ãƒãƒ¼ãƒ‰ã‚’åˆæœŸã‚°ãƒ«ãƒ¼ãƒ—åŒ–ä¸­...")
    visited = set()
    groups = {}
    node_to_group_map = {}
    group_id_counter = 0

    for node in g.vertices():
        if node not in visited:
            component = []
            q = deque([node])
            visited.add(node)
            base_cluster_id = node_to_cluster_map[node]
            
            while q:
                u = q.popleft()
                component.append(u)
                for v in g.neighbors(u):
                    if v not in visited and node_to_cluster_map.get(v) == base_cluster_id:
                        visited.add(v)
                        q.append(v)

            groups[group_id_counter] = component
            for n in component:
                node_to_group_map[n] = group_id_counter
            group_id_counter += 1
            
    print(f"      åˆæœŸã‚°ãƒ«ãƒ¼ãƒ—æ•°: {len(groups)}")
    return node_to_group_map, groups

def _merge_groups_to_target_count(g_orig_for_names: graph_tools.Graph, groups: dict, node_to_group_map: dict, embeddings: np.ndarray, target_count: int, args: argparse.Namespace) -> dict:
    """
    åˆæœŸã‚°ãƒ«ãƒ¼ãƒ—ã‚’ã€ç›®æ¨™ãƒãƒ¼ãƒ‰æ•°ã«é”ã™ã‚‹ã¾ã§ãƒãƒ¼ã‚¸ã™ã‚‹ã€‚
    â˜…ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«å¯¾ã™ã‚‹ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’è€ƒæ…®ã™ã‚‹æ©Ÿèƒ½ã‚’è¿½åŠ ã€‚
    """
    num_groups = len(groups)
    print(f"    Step 3b: åˆæœŸã‚°ãƒ«ãƒ¼ãƒ— ({num_groups}å€‹) ã‚’ç›®æ¨™ ({target_count}å€‹) ã¾ã§éšå±¤çš„ã«ãƒãƒ¼ã‚¸ä¸­...")
    
    # (Union-Find, group_embeddingsã®è¨ˆç®—ã¯å¤‰æ›´ãªã—)
    # ...
    parent = {i: i for i in groups.keys()}
    def find(i):
        if parent[i] == i:
            return i
        parent[i] = find(parent[i])
        return parent[i]
    def union(i, j):
        root_i = find(i)
        root_j = find(j)
        if root_i != root_j:
            if root_i < root_j:
                parent[root_i] = root_j
            else:
                parent[root_j] = root_i
            return True
        return False

    group_embeddings = {}
    for gid, nodes in groups.items():
        if nodes:
            group_embeddings[gid] = embeddings[nodes].mean(axis=0)


    # ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®ã‚¨ãƒƒã‚¸ã‚’åŸºã«ãƒãƒ¼ã‚¸å€™è£œã‚’è©•ä¾¡
    merge_candidates = []
    processed_pairs = set()
    for u, v in g_orig_for_names.edges():
        g1 = node_to_group_map[u]
        g2 = node_to_group_map[v]
        if g1 != g2:
            pair = tuple(sorted((g1, g2)))
            if pair not in processed_pairs:
                # å…ƒã®è·é›¢ã‚’è¨ˆç®—
                dist = np.linalg.norm(group_embeddings[g1] - group_embeddings[g2])
                
                # â˜…ãƒšãƒŠãƒ«ãƒ†ã‚£ã®è¨ˆç®—
                b_u = g_orig_for_names.get_vertex_attribute(u, 'betweenness') or 0
                b_v = g_orig_for_names.get_vertex_attribute(v, 'betweenness') or 0
                penalty = b_u * b_v
                penalized_dist = dist * (1 + args.bottleneck_penalty * penalty)

                # â˜…ãƒšãƒŠãƒ«ãƒ†ã‚£é©ç”¨å¾Œã®è·é›¢ã‚’å„ªå…ˆåº¦ã¨ã—ã¦ä½¿ç”¨
                heapq.heappush(merge_candidates, (penalized_dist, dist, g1, g2)) # å…ƒã®è·é›¢ã‚‚ãƒ­ã‚°è¡¨ç¤ºç”¨ã«ä¿æŒ
                processed_pairs.add(pair)
    
    # (ãƒãƒ¼ã‚¸çŠ¶æ³ã®è¡¨ç¤ºæº–å‚™ã¯å¤‰æ›´ãªã—)
    # ...
    id_to_orig_name = {i: g_orig_for_names.get_vertex_attribute(i, 'original_name') for i in g_orig_for_names.vertices()}
    supergroup_contents = {gid: {id_to_orig_name[node_id] for node_id in nodes} for gid, nodes in groups.items()}

    num_merges_needed = num_groups - target_count
    merges_done = 0
    print(f"      --- ãƒãƒ¼ã‚¸é–‹å§‹ (è¨ˆ {num_merges_needed} å›) ---")
    while merges_done < num_merges_needed and merge_candidates:
        # â˜…penalized_distã¨distã‚’å–å¾—
        penalized_dist, dist, g1, g2 = heapq.heappop(merge_candidates)
        
        root1 = find(g1)
        root2 = find(g2)

        if root1 != root2:
            nodes_in_sg1 = sorted(list(supergroup_contents[root1]), key=lambda x: int(x) if x.isdigit() else x)
            nodes_in_sg2 = sorted(list(supergroup_contents[root2]), key=lambda x: int(x) if x.isdigit() else x)
            
            # â˜…ãƒ­ã‚°è¡¨ç¤ºã‚’ä¿®æ­£
            print(f"      - ã‚¹ãƒ†ãƒƒãƒ— {merges_done + 1}: ä»¥ä¸‹ã®2ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ãƒãƒ¼ã‚¸ (ãƒšãƒŠãƒ«ãƒ†ã‚£å¾Œè·é›¢: {penalized_dist:.4f}, å…ƒè·é›¢: {dist:.4f})")
            print(f"        - ã‚°ãƒ«ãƒ¼ãƒ—A: {nodes_in_sg1}")
            print(f"        - ã‚°ãƒ«ãƒ¼ãƒ—B: {nodes_in_sg2}")

            union(g1, g2)
            new_root = find(g1)
            old_root = root1 if new_root == root2 else root2
            
            if new_root in supergroup_contents and old_root in supergroup_contents:
                 supergroup_contents[new_root].update(supergroup_contents[old_root])
                 del supergroup_contents[old_root]

            merges_done += 1
    print("      --- ãƒãƒ¼ã‚¸å®Œäº† ---")
    
    # (æœ€çµ‚çš„ãªãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆã¯å¤‰æ›´ãªã—)
    # ...
    final_map = {}
    supernode_roots = {find(i) for i in groups.keys()}
    root_to_final_id = {root: i for i, root in enumerate(supernode_roots)}

    for node in g_orig_for_names.vertices():
        group_id = node_to_group_map[node]
        root = find(group_id)
        final_map[node] = root_to_final_id[root]
        
    print(f"      ãƒãƒ¼ã‚¸å¾Œã®æœ€çµ‚çš„ãªã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒãƒ¼ãƒ‰æ•°: {len(supernode_roots)}")
    return final_map


def coarsen_graph_by_clustering(g_orig: graph_tools.Graph, node_map: dict, args: argparse.Namespace) -> graph_tools.Graph:
    """
    Coarsens the graph by generating embeddings, clustering nodes, and merging them.
    This version ensures that only connected nodes are merged.
    """
    # 1. Create features and embeddings
    features = create_node_features(g_orig, node_map, args.flows)
    g_nx = graph_tools_to_networkx(g_orig)
    embeddings = generate_node_embeddings(g_nx, features, args.embedding_dim, args.hidden_dim)

    if embeddings.size == 0:
        print("  åŸ‹ã‚è¾¼ã¿ãŒç”Ÿæˆã§ããªã‹ã£ãŸãŸã‚ã€å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return g_orig

    # --- Display generated embeddings for verification ---
    print("\n--- ç”Ÿæˆã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ« (æœ€åˆã®5ä»¶) ---")
    print(embeddings[:5])
    print("-" * 50)

    # 2. Cluster nodes using k-means
    num_nodes = g_orig.nvertices()
    k = int(math.ceil(args.alpha * num_nodes))
    print(f"Step 3: k-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­ (Î±={args.alpha}, å…ƒãƒãƒ¼ãƒ‰æ•°={num_nodes} -> ç›®æ¨™ã‚¯ãƒ©ã‚¹ã‚¿æ•° k={k})...")
    
    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')
    cluster_ids = kmeans.fit_predict(embeddings)
    node_to_cluster_map = {node_id: cluster_id for node_id, cluster_id in zip(sorted(g_orig.vertices()), cluster_ids)}
    
    # â˜…æ–°è¦è¿½åŠ : k-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®ç›´æ¥ã®çµæœã‚’è¡¨ç¤ºã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯
    cluster_to_nodes_map = defaultdict(list)
    for node_id, cluster_id in node_to_cluster_map.items():
        orig_name = g_orig.get_vertex_attribute(node_id, 'original_name')
        cluster_to_nodes_map[cluster_id].append(orig_name)
    
    print("\n--- k-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®ç›´æ¥ã®çµæœ ---")
    for cid, nodes in sorted(cluster_to_nodes_map.items()):
        try:
            sorted_nodes = sorted(nodes, key=int)
        except ValueError:
            sorted_nodes = sorted(nodes, key=str)
        print(f"  Cluster {cid}: {sorted_nodes}")
    print("-" * 50)
    # â˜…ã“ã“ã¾ã§ãŒæ–°è¦è¿½åŠ éƒ¨åˆ†

    # 3. Perform link-based merging
    # Step 3a: Group nodes based on connectivity within the same cluster
    node_to_group_map, groups = _group_nodes_by_connectivity_and_cluster(g_orig, node_to_cluster_map) #
    
    # Step 3b: Merge groups to meet the target coarse ratio
    target_node_count = int(math.ceil(args.alpha * num_nodes))
    if len(groups) > target_node_count:
        final_node_to_supernode_map = _merge_groups_to_target_count(g_orig, groups, node_to_group_map, embeddings, target_node_count, args) #
    else:
        print("    åˆæœŸã‚°ãƒ«ãƒ¼ãƒ—æ•°ãŒç›®æ¨™æ•°ä»¥ä¸‹ã§ã‚ã‚‹ãŸã‚ã€è¿½åŠ ãƒãƒ¼ã‚¸ã¯è¡Œã„ã¾ã›ã‚“ã€‚")
        final_node_to_supernode_map = node_to_group_map #

    # --- Display FINAL grouping results for verification ---
    supernode_to_nodes_map = defaultdict(list)
    for node_id, supernode_id in final_node_to_supernode_map.items():
        orig_name = g_orig.get_vertex_attribute(node_id, 'original_name')
        supernode_to_nodes_map[supernode_id].append(orig_name)
    
    print("\n--- æœ€çµ‚çš„ãªã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã®çµæœ ---")
    for sid, nodes in sorted(supernode_to_nodes_map.items()):
        try:
            sorted_nodes = sorted(nodes, key=int)
        except ValueError:
            sorted_nodes = sorted(nodes, key=str)
        print(f"  Supernode {sid}: {sorted_nodes}")
    print("-" * 50)

    # 4. Build the new coarse graph
    print("Step 4: ç²—è¦–åŒ–ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ä¸­...")
    g_coarse = graph_tools.Graph(directed=False)
    
    # Add supernodes to the coarse graph
    for supernode_id, orig_nodes in sorted(supernode_to_nodes_map.items()):
        g_coarse.add_vertex(supernode_id)
        
        # Determine the representative node (e.g., the one with the smallest ID)
        rep_node_name = min(orig_nodes, key=lambda x: int(x) if x.isdigit() else x)
        rep_node_id = node_map[str(rep_node_name)]
        
        child_nodes = [n for n in sorted(orig_nodes) if n != rep_node_name]
        child_node_ids = [node_map[str(n)] for n in child_nodes]
        
        g_coarse.set_vertex_attribute(supernode_id, 'rep_node_id', rep_node_id)
        if child_node_ids:
            super_attr = "".join([f"+{nid}" for nid in sorted(child_node_ids)])
            g_coarse.set_vertex_attribute(supernode_id, 'super_ids', super_attr)

    # Aggregate edges between supernodes based on the paper's rule
    print("    ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒãƒ¼ãƒ‰é–“ã®ãƒªãƒ³ã‚¯ã¨å¸¯åŸŸã‚’é›†ç´„ä¸­...")
    edge_bandwidths = defaultdict(float)
    for u_orig, v_orig in g_orig.edges():
        super_u = final_node_to_supernode_map[u_orig]
        super_v = final_node_to_supernode_map[v_orig]
        
        if super_u != super_v:
            super_edge = tuple(sorted((super_u, super_v)))
            # Rule: Sum of bandwidths of original links
            edge_bandwidths[super_edge] += DEFAULT_LINK_BANDWIDTH
    
    for (u_coarse, v_coarse), bandwidth in edge_bandwidths.items():
        g_coarse.add_edge(u_coarse, v_coarse)
        g_coarse.set_edge_weight(u_coarse, v_coarse, bandwidth)

    print("    ç²—è¦–åŒ–ã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰å®Œäº†ã€‚")
    return g_coarse


def write_coarse_graph_dot(g_coarse: graph_tools.Graph, g_orig_for_names: graph_tools.Graph, output_path: str):
    """
    æœ€çµ‚çš„ãªã‚°ãƒ©ãƒ•ã‚’å…ƒã®ãƒãƒ¼ãƒ‰åã§æ§‹ç¯‰ã—ã€DOTãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚€ã€‚
    ã‚¨ãƒƒã‚¸ãƒ©ãƒ™ãƒ«ã¯é›†ç´„ã•ã‚ŒãŸå¸¯åŸŸå¹…ã‚’è¡¨ã™ã€‚
    'random-100.dot' ã®ã‚¹ã‚¿ã‚¤ãƒ«ã«åˆã‚ã›ã¦ã€ä¸è¦ãªå¼•ç”¨ç¬¦ã‚’é¿ã‘ã‚‹ãŸã‚ã«ã€å¯èƒ½ãªå ´åˆã¯æ•´æ•°ãƒãƒ¼ãƒ‰IDã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«å‡ºåŠ›å½¢å¼ã‚’èª¿æ•´ã™ã‚‹ã€‚
    """
    print(f"\nâœ… å‡¦ç†å®Œäº†ï¼ ç²—è¦–åŒ–ã‚°ãƒ©ãƒ•ã‚’ {output_path} ã«ä¿å­˜ä¸­...")

    g_final = graph_tools.Graph(directed=False)
    coarse_id_to_final_name = {}

    def _to_int_if_possible(name_str):
        """æ–‡å­—åˆ—ãŒæœ‰åŠ¹ãªæ•´æ•°ã‚’è¡¨ã™å ´åˆã€æ–‡å­—åˆ—ã‚’æ•´æ•°ã«å¤‰æ›ã™ã‚‹ã€‚"""
        s = str(name_str)
        if s.isdigit() or (s.startswith('-') and s[1:].isdigit()):
            return int(s)
        return s

    for v_coarse in sorted(g_coarse.vertices()):
        rep_node_id = g_coarse.get_vertex_attribute(v_coarse, 'rep_node_id')
        rep_orig_name = str(g_orig_for_names.get_vertex_attribute(rep_node_id, 'original_name'))
        
        final_name = _to_int_if_possible(rep_orig_name)
        coarse_id_to_final_name[v_coarse] = final_name
        g_final.add_vertex(final_name)
        
        super_ids_attr = g_coarse.get_vertex_attribute(v_coarse, 'super_ids')
        if super_ids_attr:
            child_node_ids_str = super_ids_attr.strip('+').split('+')
            child_orig_names = []
            for child_id_str in child_node_ids_str:
                if child_id_str:
                    child_id = int(child_id_str)
                    child_orig_name = str(g_orig_for_names.get_vertex_attribute(child_id, 'original_name'))
                    child_orig_names.append(child_orig_name)
            
            if child_orig_names:
                # 'super'å±æ€§ã®å†…å®¹ã‚’æ•°å€¤çš„ã«ã‚½ãƒ¼ãƒˆã™ã‚‹
                try:
                    sorted_names = sorted(child_orig_names, key=int)
                except ValueError:
                    sorted_names = sorted(child_orig_names)
                
                final_super_attr_str = "".join([f"+{n}" for n in sorted_names])
                g_final.set_vertex_attribute(final_name, 'super', final_super_attr_str)

    for u_coarse, v_coarse in g_coarse.edges():
        u_final = coarse_id_to_final_name[u_coarse]
        v_final = coarse_id_to_final_name[v_coarse]
        if not g_final.has_edge(u_final, v_final):
             g_final.add_edge(u_final, v_final)
             bandwidth = g_coarse.get_edge_weight(u_coarse, v_coarse)
             if bandwidth:
                 g_final.set_edge_attribute_by_id(u_final, v_final, 0, 'label', int(bandwidth))

    with open(output_path, 'w') as f:
        # graph_toolsã«DOTãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã®ç”Ÿæˆã‚’ä»»ã›ã‚‹
        # g_finalã®ãƒãƒ¼ãƒ‰IDã¯å¯èƒ½ãªé™ã‚Šæ•´æ•°ã«ãªã£ã¦ã„ã‚‹ãŸã‚ã€
        # export_dotã¯ãã‚Œã‚‰ã‚’å¼•ç”¨ç¬¦ã§å›²ã¾ãšã€æœŸå¾…ã•ã‚Œã‚‹å½¢å¼ã«ä¸€è‡´ã™ã‚‹
        f.write(g_final.export_dot())

def main():
    """Main execution block."""
    parser = argparse.ArgumentParser(
        description="BW-COARSE: Max-Minå…¬å¹³æ€§ã‚’è€ƒæ…®ã—ãŸå¸¯åŸŸæ§‹é€ ä¿å­˜å‹ã‚°ãƒ©ãƒ•ç²—è¦–åŒ–",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument('-i', '--input', required=True, help='å…¥åŠ›ã‚°ãƒ©ãƒ•ã®DOTãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('-o', '--output', required=True, help='å‡ºåŠ›ã™ã‚‹ç²—è¦–åŒ–ã‚°ãƒ©ãƒ•ã®DOTãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('-f', '--flows', required=True, help='ãƒ•ãƒ­ãƒ¼å¸¯åŸŸãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('-a', '--alpha', type=float, default=0.5, help='ç²—è¦–åŒ–ç‡ (0 < alpha <= 1)')
    parser.add_argument('--embedding_dim', type=int, default=16, help='GCNãŒç”Ÿæˆã™ã‚‹åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°')
    parser.add_argument('--hidden_dim', type=int, default=32, help='GCNä¸­é–“å±¤ã®æ¬¡å…ƒæ•°')
    parser.add_argument('--bottleneck_penalty', type=float, default=10000, help='ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãƒªãƒ³ã‚¯ã®é›†ç´„ã‚’é˜²ããŸã‚ã®ãƒšãƒŠãƒ«ãƒ†ã‚£ä¿‚æ•°ã€‚å¤§ãã„ã»ã©åŠ¹æœãŒå¼·ã„ã€‚')
    args = parser.parse_args()    

    if not (0 < args.alpha <= 1):
        parser.error("ç²—è¦–åŒ–ç‡ alpha ã¯ 0 ã‚ˆã‚Šå¤§ãã 1 ä»¥ä¸‹ã®å€¤ã§ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚")

    print("ğŸ”· BW-COARSE ãƒ—ãƒ­ã‚°ãƒ©ãƒ é–‹å§‹ ğŸ”·")
    
    g_tools_orig, node_map = import_graph(args.input)
    
    g_coarse = coarsen_graph_by_clustering(g_tools_orig, node_map, args)
    
    write_coarse_graph_dot(g_coarse, g_tools_orig, args.output)
    
    print("-" * 50)
    print("ã‚µãƒãƒªãƒ¼:")
    print(f"  å…ƒã®ãƒãƒ¼ãƒ‰æ•°: {g_tools_orig.nvertices()}")
    print(f"  ç²—è¦–åŒ–å¾Œã®ãƒãƒ¼ãƒ‰æ•°: {g_coarse.nvertices()}")
    print(f"  ãƒ•ã‚¡ã‚¤ãƒ« '{args.output}' ãŒä½œæˆã•ã‚Œã¾ã—ãŸã€‚")
    print("-" * 50)

if __name__ == "__main__":
    main()